---
title: 'Hierarchical Clustering Algorithms for large data sets'
layout: post
tags:
  - Clustering
  - Data-Mining
category: Notes
mathjax: true
---

Since the number of operations required by hierarchical agglomerative methods are $O(N^3)$ or at least $O(N^2)$, where $N$ is the number of data points, a well-designed hierarchical agglomerative alogorithm is needed when the dataset we want to apply is large.

In this document, we will introduce **CURE (Clustering Using Representation)** and explain how to use Hierarchical Agglomerative Clustering to deal with noisy image.

<!--more-->

## The CURE (Clustering Using Representation) Algorithm

In CURE (Clustering Using Representation), each cluster $C$ is represented by $R_C$, which is a set of representatives (i.e., $\|R_C\| = \kappa = 10$ for each cluster $C$).   These representatives are chosen on the border of the cluster, trying to capture its shape.   Then, they are pushed towards cluster mean, in order to discard the irregularities of the border.

More specifically, to get representatives $R_C$ from cluster $C$ (each data point is a cluster initially), the following steps are performed.

**Step 0.** If $\|C\| \leq \kappa$, then just let $R_C$ be $C$ and skip all remaining steps.

**Step 1.** Select an arbitrary $x_1 \in C$, with the maximum distance from the mean of $C$. Let $R_C =$ {$x_1$}

**Step 2.** for $i = 2,3,..., \kappa$, Pick a $x_i \in C-R_C$ that lies fartherest from points in $R_C$ and then let $R_C = R_C \cup$ {$x_i$}

**Step 3.** Shrink all points of $R_C$ towards the mean ${mean}_C$ by a given factor $a$. ($0 \leq a \leq 1$)

$$
x = a \cdot {mean}_C + (1-a) \cdot x
$$

The distance between 2 clusters is

$$
d(C_i, C_j) = \min_{x \in R_{C_i}, y \in R_{C_j}} d(x, y)
$$

To save time, when merging 2 cluster and get the $Union = C_1 \cup C_2$, the $\kappa$ points of $R_{Union}$ are chosen from the (at most) $\kappa + \kappa = 2 \kappa$ points in $R_{C_1}$ and $R_{C_2}.$



### CURE Utilizing Random Sampling

In 1998,  Guha et. al proved that the time complexity of CURE is $O(N^2log_2N)$ in the worst case, which is still time comsuming.

That's why random sampling is utilized here to save time.   In other words, the adoption of random sampling makes $N$ into $N'$ and the time complexity becomes $O(N'^2log_2N')$.   However, the sampled data $X'$ should be sufficient enough.

**Step 1.** Divide data set $X$ randomly into $p$ sample data sets ($p=N/N'$, each set has $N'$ data points).

**Step 2.** For each sample data set, apply the original version of CURE, until (at most) $N'/q$ clusters are formed ($q>1$).

**Step 3.** Consider all $(N'/q) \times p$ clusters, merge similar clusters to obtain $k$ clusters.


### Hyper-parameters in CURE

Note that CURE with Random Sampling is sensitive to $\kappa$, $N'$, and $a$.

- $\kappa$ must be large enough to capture the geometry of each cluster
- $N'$ must be higher than a certain percentage of $N$. Typically $N' \geq 0.025N = 1/40 \times N$
- If $a$ is small, then CURE behaves like Minimum Spanning Tree. If $a$ is large it resembles to the algorithm that use a single point repersentative for each cluster.

## Using Hierarchical Agglomerative Clustering to Deal with Noisy Image

Suppose the matrix below is a block in a gray-scale image.


$$
\begin{bmatrix}
22 & 33 & 44\\
239 & x & 235 \\
238 & 237 & 236
\end{bmatrix}
$$

If a pixel whose gray value is $x$, then how to check if $x$ is a noise and how to impute it?


**Step 1. Hierarchical agglomerative clustering on 8 neighbors**

Suppose we have defined a threshold called "Big Jump".   We can use it to determine whether an agglomerative merge should be done or not.   The number of clusters are determined automatically hence.

For example, in this case we will obtain 2 clusters $C_1$ and $C_2$ (along with their clsuter centers $y_1$ and $y_2$) after performing hierarchical clustering on all 8 neighbors of $x$.

- $C_1$ = { $22,33,44$ }, $y_1 = 33$
- $C_2$ = { $235, 236, 237, 238, 239$ }, $y_2 = 237$

$C_1$ and $C_2$ will not be merged since the distance between them is too large.

**Step 2. Is $x$ a noise? How far away is $x$ from $y_1$? from $y_2$?**

$$
\|x-y_1\| < \theta~~~\text{or}~~~\|x-y_2\| < \theta
\\
\rightarrow x \notin Noise
\\
~
\\
\theta~\text{: noise-tolerance threshold}
$$


**Step 3. If $x$ is a noise, then replace $x$ by $y_1$ or $y_2$.**

For example, in this case we replace $x$ with $y_2$ since $\|C_2\| > \|C_1\|$.

If more than 1 cluster has the largest number of members, use the one with more $N$/$E$/$S$/$W$ member(s).

$$
\begin{bmatrix}
22 & (N=33)  & 44\\
(W=239) & x & (E=235) \\
238 & (S=237) & 236
\end{bmatrix}
$$

### Experimental Results and Discussions

Experiment settings are

- percentage of contanminated pixels: 10%, 25%, or 50%.
- standard deviation of the contanmination distribution: 20, 40, or 80.
- The agglomerative jump threshold: 15 or 25.
- Noise-tolerance threshold: 20, 36.

<!--| Factor | Level | Average RMS |
| - | - | - |
|  |  | 10.71
|  |  | 14.82-->

And from the experimental results (which you can find in the paper), it is concluded that the factor **"noise-tolerance"** is significant and the agglomerative jump is not so significant.

On the other hand, if k-means is used instead of agglomerative clustering, then the imputed image looks bad; if median filter (always use the median of neighbors to replace $x$) is used then the imputed image looks smooth but blurry.

Another benefit is that this algorithm can also be used for edge detection.   However, it does not perform well if there exists 2 adjacent noises with similar values.   In this case, the algorithm will interpret them as a cluster and leave them untouched.

## References

- [Guha, S., Rastogi, R., & Shim, K. (1998, June). CURE: an efficient clustering algorithm for large databases. In ACM Sigmod Record (Vol. 27, No. 2, pp. 73-84). ACM.](https://dl.acm.org/citation.cfm?id=276312)
- [Abreu, E., Lightstone, M., Mitra, S. K., & Arakawa, K. (1996). A new efficient approach for the removal of impulse noise from highly corrupted images. IEEE transactions on image processing, 5(6), 1012-1025.](http://ieeexplore.ieee.org/abstract/document/503916/)