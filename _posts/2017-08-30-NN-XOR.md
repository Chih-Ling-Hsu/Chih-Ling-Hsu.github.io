---
title: 'Code Example of a Neural Network for The Function XOR'
layout: post
tags:
  - Python
  - Neural Network
  - Data Mining
category: Programming
mathjax: true
---

It is a well-known fact, and something we have already mentioned, that 1-layer neural networks cannot predict the function XOR. 1-layer neural nets can only classify linearly separable sets, however, as we have seen, the Universal Approximation Theorem states that a 2-layer network can approximate any function, given a complex enough architecture. 

We will now create a neural network with two neurons in the hidden layer and we will show how this can model the XOR function. However, we will write code that will allow the reader to simply modify it to allow for any number of layers and neurons in each layer, so that the reader can try simulating different scenarios. We are also going to use the hyperbolic tangent as the activity function for this network. To train the network, we will implement the back-propagation algorithm discussed earlier.

<!--more-->

## Define the Neural Network Class

We will need to import the library, `numpy`, though if the reader wished to visualize the results, we also recommend importing `matplotlib`.


```python
import numpy
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt

# The following code is used for hiding the warnings and make this notebook clearer.
import warnings
warnings.filterwarnings('ignore')
```

Next we define our activity function and its derivative (we use `tanh(x)` in this example):


```python
def tanh(x):
    return (1.0 - numpy.exp(-2*x))/(1.0 + numpy.exp(-2*x))

def tanh_derivative(x):
    return (1 + tanh(x))*(1 - tanh(x))
```

Next we define the `NeuralNetwork` class:


```python
class NeuralNetwork:
    #########
    # Since we are studying the XOR function, 
    # for the input layer we need to have two neurons, 
    # and for the output layer only one neuron.
    #
    #
    # parameters
    # ----------
    # self:      the class object itself
    # net_arch:  consists of a list of integers, indicating
    #            the number of neurons in each layer, i.e. the network architecture
    #########
    def __init__(self, net_arch):
        
        # initialized the weights, making sure we also 
        # initialize the weights for the biases that we will add later
        self.activity = tanh
        self.activity_derivative = tanh_derivative
        self.layers = len(net_arch)
        self.steps_per_epoch = 1000
        self.arch = net_arch
        self.weights = []

        # range of weight values (-1,1)
        for layer in range(self.layers - 1):
            w = 2*numpy.random.rand(net_arch[layer] + 1, net_arch[layer+1]) - 1
            self.weights.append(w)
    
    #########
    # the fit function will train our network. In the last line, 
    # `nn` represents the `NeuralNetwork` class and `predict` is the function in the NeuralNetwork class 
    # that we will define later.
    # 
    # parameters
    # ----------
    # self:    the class object itself
    # data:    the set of all possible pairs of booleans True or False indicated by the integers 1 or 0
    # labels:  the result of the logical operation 'xor' on each of those input pairs
    #########
    def fit(self, data, labels, learning_rate=0.1, epochs=100):
        
        # Add bias units to the input layer - 
        # add a "1" to the input data (the always-on bias neuron)
        ones = numpy.ones((1, data.shape[0]))
        Z = numpy.concatenate((ones.T, data), axis=1)
        
        training = epochs*self.steps_per_epoch
        for k in range(training):
            if k % self.steps_per_epoch == 0:
                '''print('epochs: {}'.format(k/self.steps_per_epoch))
                for s in data:
                    print(s, nn.predict_single_data(s))'''
        
            # We will now go ahead and set up our feed-forward propagation:
            sample = numpy.random.randint(data.shape[0])
            y = [Z[sample]]
            for i in range(len(self.weights)-1):
                activation = numpy.dot(y[i], self.weights[i])
                activity = self.activity(activation)

                # add the bias for the next layer
                activity = numpy.concatenate((numpy.ones(1), numpy.array(activity)))
                y.append(activity)

            # last layer
            activation = numpy.dot(y[-1], self.weights[-1])
            activity = self.activity(activation)
            y.append(activity)


            # Now we do our back-propagation of the error to adjust the weights:
            error = labels[sample] - y[-1]
            delta_vec = [error * self.activity_derivative(y[-1])]

            # we need to begin from the back, from the next to last layer
            for i in range(self.layers-2, 0, -1):
                error = delta_vec[-1].dot(self.weights[i][1:].T)
                error = error*self.activity_derivative(y[i][1:])
                delta_vec.append(error)

            # Now we need to set the values from back to front
            delta_vec.reverse()

            # Finally, we adjust the weights, using the backpropagation rules
            for i in range(len(self.weights)):
                layer = y[i].reshape(1, nn.arch[i]+1)
                delta = delta_vec[i].reshape(1, nn.arch[i+1])
                self.weights[i] += learning_rate*layer.T.dot(delta)
    
    #########
    # the predict function is used to check the prediction result of
    # this neural network.
    # 
    # parameters
    # ----------
    # self:   the class object itself
    # x:      single input data
    #########
    def predict_single_data(self, x):
        val = numpy.concatenate((numpy.ones(1).T, numpy.array(x)))
        for i in range(0, len(self.weights)):
            val = self.activity(numpy.dot(val, self.weights[i]))
            val = numpy.concatenate((numpy.ones(1).T, numpy.array(val)))
        return val[1]
    
    #########
    # the predict function is used to check the prediction result of
    # this neural network.
    # 
    # parameters
    # ----------
    # self:   the class object itself
    # x:      the input data array
    #########
    def predict(self, X):
        Y = None
        for x in X:
            y = numpy.array([[self.predict_single_data(x)]])
            if Y==None:
                Y = y
            else:
                Y = numpy.vstack((Y,y))
        return Y
```

## Check the Performance of This Neural Network

Now we can check if this Neural Network can actually learn `XOR` rule, which is

|  | y=0 | y=1 |
| - | - | - |
| **x=0** | 0 | 1 |
| **x=1** | 1 | 0 |



```python
numpy.random.seed(0)

# Initialize the NeuralNetwork with
# 2 input neurons
# 2 hidden neurons
# 1 output neuron
nn = NeuralNetwork([2,2,1])

# Set the input data
X = numpy.array([[0, 0], [0, 1],
                [1, 0], [1, 1]])

# Set the labels, the correct results for the xor operation
y = numpy.array([0, 1, 
                 1, 0])

# Call the fit function and train the network for a chosen number of epochs
nn.fit(X, y, epochs=10)

# Show the prediction results
print("Final prediction")
for s in X:
    print(s, nn.predict_single_data(s))

```

    Final prediction
    [0 0] 0.0030321736925
    [0 1] 0.996386076136
    [1 0] 0.995903456394
    [1 1] 0.000638644921757
    

The reader can slightly modify the code we created in the `plot_decision_regions` function used earlier in this book and see how different neural networks separate different regions depending on the architecture chosen.


```python
numpy.random.seed(0)
nn = NeuralNetwork([2,2,1])
nn.fit(X, y, epochs=10)
plot_decision_regions(X, y, nn)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```


![](http://i.imgur.com/oe0z0op.png)




Different neural network architectures (for example, implementing a network with a different number of neurons in the hidden layer, or with more than just one hidden layer) may produce a different separating region.

For example, `([2,4,3,1])` will represent a 3-layer neural network, with four neurons in the first hidden layer and three neurons in the second hidden layer, and choosing it will give the following figure:


```python
numpy.random.seed(0)
nn = NeuralNetwork([2,4,3,1])
nn.fit(X, y, epochs=10)
plot_decision_regions(X, y, nn)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```


![](http://i.imgur.com/2OKU3nx.png)


While choosing `nn = NeuralNetwork([2,4,1])`, for example, would produce the following:


```python
numpy.random.seed(0)
nn = NeuralNetwork([2,4,1])
nn.fit(X, y, epochs=10)
plot_decision_regions(X, y, nn)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()
```


![](http://i.imgur.com/CCNCVj7.png)


## Appendix

The self-defined plot functions are written here.


```python
import numpy as np
def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):

    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.8, c=cmap(idx),
                    marker=markers[idx], label=cl)

    # highlight test samples
    if test_idx:
        # plot all samples
        X_test, y_test = X[test_idx, :], y[test_idx]

        plt.scatter(X_test[:, 0],
                    X_test[:, 1],
                    c='',
                    alpha=1.0,
                    linewidths=1,
                    marker='o',
                    s=55, label='test set')
```

## References
- [“Python Deep Learning,” by Valentino Zocca, Gianmario Spacagna, Daniel Slater, Peter Roelants.](https://www.amazon.com/Python-Deep-Learning-Valentino-Zocca/dp/1786464454)